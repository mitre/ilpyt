<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ilpyt.agents.gail_agent API documentation</title>
<meta name="description" content="An implementation of the agent from the Generative Adversarial Imitation
Learning (GAIL) algorithm. This algorithm was described in the paper â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ilpyt.agents.gail_agent</code></h1>
</header>
<section id="section-intro">
<p>An implementation of the agent from the Generative Adversarial Imitation
Learning (GAIL) algorithm. This algorithm was described in the paper "Generative
Adversarial Imitation Learning" by Jonathan Ho and Stefano Ermon, and presented
at NIPS 2016.</p>
<p>For more details, please refer to the paper: <a href="https://arxiv.org/abs/1606.03476">https://arxiv.org/abs/1606.03476</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
An implementation of the agent from the Generative Adversarial Imitation 
Learning (GAIL) algorithm. This algorithm was described in the paper &#34;Generative 
Adversarial Imitation Learning&#34; by Jonathan Ho and Stefano Ermon, and presented 
at NIPS 2016.

For more details, please refer to the paper: https://arxiv.org/abs/1606.03476
&#34;&#34;&#34;

from typing import Dict, Union

import numpy as np
import torch
import torch.nn.functional as F
from torch.optim import Adam

from ilpyt.agents.a2c_agent import A2CAgent
from ilpyt.agents.base_agent import BaseAgent
from ilpyt.agents.dqn_agent import DQNAgent
from ilpyt.agents.ppo_agent import PPOAgent
from ilpyt.nets.base_net import BaseNetwork
from ilpyt.utils.agent_utils import flatten_tensor


class GAILAgent(BaseAgent):
    def initialize(
        self,
        gen: Union[BaseAgent, None] = None,
        disc: Union[BaseNetwork, None] = None,
        lr: float = 1e-4,
    ) -&gt; None:
        &#34;&#34;&#34;
        Initialization function for the GAIL agent.

        Parameters
        ----------
        gen: BaseAgent, default=None
            actor (policy) network
        disc: BaseNetwork, default=None
            discriminator network
        lr: float, default=1e-4
            learning rate

        Raises
        ------
        ValueError:
            if `gen` is not specified, or is not an RL Agent (A2CAgent, 
            DQNAgent, or PPOAgent) or `disc` is not specified
        &#34;&#34;&#34;
        # Networks
        if gen is None:
            raise ValueError(
                &#39;Please provide input value for gen. Currently set to None.&#39;
            )
        if disc is None:
            raise ValueError(
                &#39;Please provide input value for disc. Currently set to None.&#39;
            )
        if (
            not isinstance(gen, A2CAgent)
            and not isinstance(gen, DQNAgent)
            and not isinstance(gen, PPOAgent)
        ):
            raise ValueError(
                &#39;GAILAgent.gen is only compatible with A2C, DQN, and PPO agents.&#39;
            )
        self.gen = gen
        self.disc = disc
        self.nets = {&#39;disc&#39;: self.disc, **self.gen.nets}
        self.opt_disc = Adam(self.disc.parameters(), lr)

    @torch.no_grad()
    def step(self, state: torch.Tensor) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Find best action for the given state according to the current policy.

        Parameters
        ----------
        state: torch.Tensor
            state tensor, of size (batch_size, state_shape)

        Returns
        -------
        np.ndarray:
            selected actions, of size (batch_size, action_shape)
        &#34;&#34;&#34;
        return self.gen.step(state)

    def update(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, float]:
        &#34;&#34;&#34;
        Update agent policy based on batch of experiences.

        Parameters
        ----------
        batch: Dict[str, torch.Tensor]
            batch of transitions, with keys `states`, `actions`, 
            `expert_states`, and `expert_actions`. Values should be of size 
            (num_steps, num_env, item_shape)

        Returns
        -------
        Dict[str, float]:
            losses for the update step, key strings and loss values can be 
            automatically recorded to TensorBoard
        &#34;&#34;&#34;
        # Rewards
        rollout_steps = batch[&#39;states&#39;].shape[0]
        with torch.no_grad():
            rewards = []
            for i in range(rollout_steps):
                logits = torch.sigmoid(
                    self.disc(batch[&#39;states&#39;][i], batch[&#39;actions&#39;][i])
                )
                reward = -torch.log(logits)
                rewards.append(reward.squeeze())
            rewards = torch.stack(rewards)

        # Update discriminator
        learner_logits = self.disc(
            flatten_tensor(batch[&#39;states&#39;]), flatten_tensor(batch[&#39;actions&#39;])
        ).squeeze()
        expert_logits = self.disc(
            batch[&#39;expert_states&#39;], batch[&#39;expert_actions&#39;]
        ).squeeze()
        loss_disc = F.binary_cross_entropy_with_logits(
            learner_logits, torch.ones_like(learner_logits)
        ) + F.binary_cross_entropy_with_logits(
            expert_logits, torch.zeros_like(expert_logits)
        )
        self.opt_disc.zero_grad()
        loss_disc.backward(retain_graph=True)
        torch.nn.utils.clip_grad_norm_(self.disc.parameters(), 1.5)
        self.opt_disc.step()

        # Update generator
        batch[&#39;rewards&#39;] = rewards
        loss_gen_dict = self.gen.update(batch)

        # Return loss dictionary
        loss_dict = {
            &#39;loss/disc&#39;: loss_disc.item(),
            &#39;loss/gen&#39;: loss_gen_dict[&#39;loss/total&#39;],
            &#39;loss/total&#39;: loss_disc.item() + loss_gen_dict[&#39;loss/total&#39;],
        }
        return loss_dict

    def to_gpu(self) -&gt; None:
        &#34;&#34;&#34;
        Place agent nets on the GPU.
        &#34;&#34;&#34;
        super(GAILAgent, self).to_gpu()
        self.gen.to_gpu()

    def to_cpu(self) -&gt; None:
        &#34;&#34;&#34;
        Place agent nets on the CPU.
        &#34;&#34;&#34;
        super(GAILAgent, self).to_cpu()
        self.gen.to_cpu()

    def set_train(self) -&gt; None:
        &#34;&#34;&#34;
        Set agent nets to training mode.
        &#34;&#34;&#34;
        super(GAILAgent, self).set_train()
        self.gen.set_train()

    def set_test(self) -&gt; None:
        &#34;&#34;&#34;
        Set agent nets to evaluation mode.
        &#34;&#34;&#34;
        super(GAILAgent, self).set_test()
        self.gen.set_test()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ilpyt.agents.gail_agent.GAILAgent"><code class="flex name class">
<span>class <span class="ident">GAILAgent</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>By default, the agent will be in <code>train</code> mode and be configured to use
the <code>cpu</code> for <code>step</code> and <code>update</code> calls.</p>
<h2 id="parameters">Parameters</h2>
<p>**kwargs:
arbitrary keyword arguments that will be passed to the <code>initialize</code> function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GAILAgent(BaseAgent):
    def initialize(
        self,
        gen: Union[BaseAgent, None] = None,
        disc: Union[BaseNetwork, None] = None,
        lr: float = 1e-4,
    ) -&gt; None:
        &#34;&#34;&#34;
        Initialization function for the GAIL agent.

        Parameters
        ----------
        gen: BaseAgent, default=None
            actor (policy) network
        disc: BaseNetwork, default=None
            discriminator network
        lr: float, default=1e-4
            learning rate

        Raises
        ------
        ValueError:
            if `gen` is not specified, or is not an RL Agent (A2CAgent, 
            DQNAgent, or PPOAgent) or `disc` is not specified
        &#34;&#34;&#34;
        # Networks
        if gen is None:
            raise ValueError(
                &#39;Please provide input value for gen. Currently set to None.&#39;
            )
        if disc is None:
            raise ValueError(
                &#39;Please provide input value for disc. Currently set to None.&#39;
            )
        if (
            not isinstance(gen, A2CAgent)
            and not isinstance(gen, DQNAgent)
            and not isinstance(gen, PPOAgent)
        ):
            raise ValueError(
                &#39;GAILAgent.gen is only compatible with A2C, DQN, and PPO agents.&#39;
            )
        self.gen = gen
        self.disc = disc
        self.nets = {&#39;disc&#39;: self.disc, **self.gen.nets}
        self.opt_disc = Adam(self.disc.parameters(), lr)

    @torch.no_grad()
    def step(self, state: torch.Tensor) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Find best action for the given state according to the current policy.

        Parameters
        ----------
        state: torch.Tensor
            state tensor, of size (batch_size, state_shape)

        Returns
        -------
        np.ndarray:
            selected actions, of size (batch_size, action_shape)
        &#34;&#34;&#34;
        return self.gen.step(state)

    def update(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, float]:
        &#34;&#34;&#34;
        Update agent policy based on batch of experiences.

        Parameters
        ----------
        batch: Dict[str, torch.Tensor]
            batch of transitions, with keys `states`, `actions`, 
            `expert_states`, and `expert_actions`. Values should be of size 
            (num_steps, num_env, item_shape)

        Returns
        -------
        Dict[str, float]:
            losses for the update step, key strings and loss values can be 
            automatically recorded to TensorBoard
        &#34;&#34;&#34;
        # Rewards
        rollout_steps = batch[&#39;states&#39;].shape[0]
        with torch.no_grad():
            rewards = []
            for i in range(rollout_steps):
                logits = torch.sigmoid(
                    self.disc(batch[&#39;states&#39;][i], batch[&#39;actions&#39;][i])
                )
                reward = -torch.log(logits)
                rewards.append(reward.squeeze())
            rewards = torch.stack(rewards)

        # Update discriminator
        learner_logits = self.disc(
            flatten_tensor(batch[&#39;states&#39;]), flatten_tensor(batch[&#39;actions&#39;])
        ).squeeze()
        expert_logits = self.disc(
            batch[&#39;expert_states&#39;], batch[&#39;expert_actions&#39;]
        ).squeeze()
        loss_disc = F.binary_cross_entropy_with_logits(
            learner_logits, torch.ones_like(learner_logits)
        ) + F.binary_cross_entropy_with_logits(
            expert_logits, torch.zeros_like(expert_logits)
        )
        self.opt_disc.zero_grad()
        loss_disc.backward(retain_graph=True)
        torch.nn.utils.clip_grad_norm_(self.disc.parameters(), 1.5)
        self.opt_disc.step()

        # Update generator
        batch[&#39;rewards&#39;] = rewards
        loss_gen_dict = self.gen.update(batch)

        # Return loss dictionary
        loss_dict = {
            &#39;loss/disc&#39;: loss_disc.item(),
            &#39;loss/gen&#39;: loss_gen_dict[&#39;loss/total&#39;],
            &#39;loss/total&#39;: loss_disc.item() + loss_gen_dict[&#39;loss/total&#39;],
        }
        return loss_dict

    def to_gpu(self) -&gt; None:
        &#34;&#34;&#34;
        Place agent nets on the GPU.
        &#34;&#34;&#34;
        super(GAILAgent, self).to_gpu()
        self.gen.to_gpu()

    def to_cpu(self) -&gt; None:
        &#34;&#34;&#34;
        Place agent nets on the CPU.
        &#34;&#34;&#34;
        super(GAILAgent, self).to_cpu()
        self.gen.to_cpu()

    def set_train(self) -&gt; None:
        &#34;&#34;&#34;
        Set agent nets to training mode.
        &#34;&#34;&#34;
        super(GAILAgent, self).set_train()
        self.gen.set_train()

    def set_test(self) -&gt; None:
        &#34;&#34;&#34;
        Set agent nets to evaluation mode.
        &#34;&#34;&#34;
        super(GAILAgent, self).set_test()
        self.gen.set_test()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ilpyt.agents.base_agent.BaseAgent" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent">BaseAgent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ilpyt.agents.gail_agent.GAILAgent.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, gen:Â Union[<a title="ilpyt.agents.base_agent.BaseAgent" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent">BaseAgent</a>,Â NoneType]Â =Â None, disc:Â Union[<a title="ilpyt.nets.base_net.BaseNetwork" href="../nets/base_net.html#ilpyt.nets.base_net.BaseNetwork">BaseNetwork</a>,Â NoneType]Â =Â None, lr:Â floatÂ =Â 0.0001) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Initialization function for the GAIL agent.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>gen</code></strong> :&ensp;<code>BaseAgent</code>, default=<code>None</code></dt>
<dd>actor (policy) network</dd>
<dt><strong><code>disc</code></strong> :&ensp;<code>BaseNetwork</code>, default=<code>None</code></dt>
<dd>discriminator network</dd>
<dt><strong><code>lr</code></strong> :&ensp;<code>float</code>, default=<code>1e-4</code></dt>
<dd>learning rate</dd>
</dl>
<h2 id="raises">Raises</h2>
<h2 id="valueerror">Valueerror</h2>
<p>if <code>gen</code> is not specified, or is not an RL Agent (A2CAgent,
DQNAgent, or PPOAgent) or <code>disc</code> is not specified</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(
    self,
    gen: Union[BaseAgent, None] = None,
    disc: Union[BaseNetwork, None] = None,
    lr: float = 1e-4,
) -&gt; None:
    &#34;&#34;&#34;
    Initialization function for the GAIL agent.

    Parameters
    ----------
    gen: BaseAgent, default=None
        actor (policy) network
    disc: BaseNetwork, default=None
        discriminator network
    lr: float, default=1e-4
        learning rate

    Raises
    ------
    ValueError:
        if `gen` is not specified, or is not an RL Agent (A2CAgent, 
        DQNAgent, or PPOAgent) or `disc` is not specified
    &#34;&#34;&#34;
    # Networks
    if gen is None:
        raise ValueError(
            &#39;Please provide input value for gen. Currently set to None.&#39;
        )
    if disc is None:
        raise ValueError(
            &#39;Please provide input value for disc. Currently set to None.&#39;
        )
    if (
        not isinstance(gen, A2CAgent)
        and not isinstance(gen, DQNAgent)
        and not isinstance(gen, PPOAgent)
    ):
        raise ValueError(
            &#39;GAILAgent.gen is only compatible with A2C, DQN, and PPO agents.&#39;
        )
    self.gen = gen
    self.disc = disc
    self.nets = {&#39;disc&#39;: self.disc, **self.gen.nets}
    self.opt_disc = Adam(self.disc.parameters(), lr)</code></pre>
</details>
</dd>
<dt id="ilpyt.agents.gail_agent.GAILAgent.set_test"><code class="name flex">
<span>def <span class="ident">set_test</span></span>(<span>self) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Set agent nets to evaluation mode.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_test(self) -&gt; None:
    &#34;&#34;&#34;
    Set agent nets to evaluation mode.
    &#34;&#34;&#34;
    super(GAILAgent, self).set_test()
    self.gen.set_test()</code></pre>
</details>
</dd>
<dt id="ilpyt.agents.gail_agent.GAILAgent.set_train"><code class="name flex">
<span>def <span class="ident">set_train</span></span>(<span>self) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Set agent nets to training mode.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_train(self) -&gt; None:
    &#34;&#34;&#34;
    Set agent nets to training mode.
    &#34;&#34;&#34;
    super(GAILAgent, self).set_train()
    self.gen.set_train()</code></pre>
</details>
</dd>
<dt id="ilpyt.agents.gail_agent.GAILAgent.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, state:Â torch.Tensor) â€‘>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Find best action for the given state according to the current policy.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>state tensor, of size (batch_size, state_shape)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray:</code></dt>
<dd>selected actions, of size (batch_size, action_shape)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def step(self, state: torch.Tensor) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Find best action for the given state according to the current policy.

    Parameters
    ----------
    state: torch.Tensor
        state tensor, of size (batch_size, state_shape)

    Returns
    -------
    np.ndarray:
        selected actions, of size (batch_size, action_shape)
    &#34;&#34;&#34;
    return self.gen.step(state)</code></pre>
</details>
</dd>
<dt id="ilpyt.agents.gail_agent.GAILAgent.to_cpu"><code class="name flex">
<span>def <span class="ident">to_cpu</span></span>(<span>self) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Place agent nets on the CPU.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_cpu(self) -&gt; None:
    &#34;&#34;&#34;
    Place agent nets on the CPU.
    &#34;&#34;&#34;
    super(GAILAgent, self).to_cpu()
    self.gen.to_cpu()</code></pre>
</details>
</dd>
<dt id="ilpyt.agents.gail_agent.GAILAgent.to_gpu"><code class="name flex">
<span>def <span class="ident">to_gpu</span></span>(<span>self) â€‘>Â NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Place agent nets on the GPU.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_gpu(self) -&gt; None:
    &#34;&#34;&#34;
    Place agent nets on the GPU.
    &#34;&#34;&#34;
    super(GAILAgent, self).to_gpu()
    self.gen.to_gpu()</code></pre>
</details>
</dd>
<dt id="ilpyt.agents.gail_agent.GAILAgent.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, batch:Â Dict[str,Â torch.Tensor]) â€‘>Â Dict[str,Â float]</span>
</code></dt>
<dd>
<div class="desc"><p>Update agent policy based on batch of experiences.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>batch</code></strong> :&ensp;<code>Dict[str, torch.Tensor]</code></dt>
<dd>batch of transitions, with keys <code>states</code>, <code>actions</code>,
<code>expert_states</code>, and <code>expert_actions</code>. Values should be of size
(num_steps, num_env, item_shape)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, float]:</code></dt>
<dd>losses for the update step, key strings and loss values can be
automatically recorded to TensorBoard</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, float]:
    &#34;&#34;&#34;
    Update agent policy based on batch of experiences.

    Parameters
    ----------
    batch: Dict[str, torch.Tensor]
        batch of transitions, with keys `states`, `actions`, 
        `expert_states`, and `expert_actions`. Values should be of size 
        (num_steps, num_env, item_shape)

    Returns
    -------
    Dict[str, float]:
        losses for the update step, key strings and loss values can be 
        automatically recorded to TensorBoard
    &#34;&#34;&#34;
    # Rewards
    rollout_steps = batch[&#39;states&#39;].shape[0]
    with torch.no_grad():
        rewards = []
        for i in range(rollout_steps):
            logits = torch.sigmoid(
                self.disc(batch[&#39;states&#39;][i], batch[&#39;actions&#39;][i])
            )
            reward = -torch.log(logits)
            rewards.append(reward.squeeze())
        rewards = torch.stack(rewards)

    # Update discriminator
    learner_logits = self.disc(
        flatten_tensor(batch[&#39;states&#39;]), flatten_tensor(batch[&#39;actions&#39;])
    ).squeeze()
    expert_logits = self.disc(
        batch[&#39;expert_states&#39;], batch[&#39;expert_actions&#39;]
    ).squeeze()
    loss_disc = F.binary_cross_entropy_with_logits(
        learner_logits, torch.ones_like(learner_logits)
    ) + F.binary_cross_entropy_with_logits(
        expert_logits, torch.zeros_like(expert_logits)
    )
    self.opt_disc.zero_grad()
    loss_disc.backward(retain_graph=True)
    torch.nn.utils.clip_grad_norm_(self.disc.parameters(), 1.5)
    self.opt_disc.step()

    # Update generator
    batch[&#39;rewards&#39;] = rewards
    loss_gen_dict = self.gen.update(batch)

    # Return loss dictionary
    loss_dict = {
        &#39;loss/disc&#39;: loss_disc.item(),
        &#39;loss/gen&#39;: loss_gen_dict[&#39;loss/total&#39;],
        &#39;loss/total&#39;: loss_disc.item() + loss_gen_dict[&#39;loss/total&#39;],
    }
    return loss_dict</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ilpyt.agents.base_agent.BaseAgent" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent">BaseAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="ilpyt.agents.base_agent.BaseAgent.load" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent.load">load</a></code></li>
<li><code><a title="ilpyt.agents.base_agent.BaseAgent.save" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent.save">save</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ilpyt.agents" href="index.html">ilpyt.agents</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ilpyt.agents.gail_agent.GAILAgent" href="#ilpyt.agents.gail_agent.GAILAgent">GAILAgent</a></code></h4>
<ul class="two-column">
<li><code><a title="ilpyt.agents.gail_agent.GAILAgent.initialize" href="#ilpyt.agents.gail_agent.GAILAgent.initialize">initialize</a></code></li>
<li><code><a title="ilpyt.agents.gail_agent.GAILAgent.set_test" href="#ilpyt.agents.gail_agent.GAILAgent.set_test">set_test</a></code></li>
<li><code><a title="ilpyt.agents.gail_agent.GAILAgent.set_train" href="#ilpyt.agents.gail_agent.GAILAgent.set_train">set_train</a></code></li>
<li><code><a title="ilpyt.agents.gail_agent.GAILAgent.step" href="#ilpyt.agents.gail_agent.GAILAgent.step">step</a></code></li>
<li><code><a title="ilpyt.agents.gail_agent.GAILAgent.to_cpu" href="#ilpyt.agents.gail_agent.GAILAgent.to_cpu">to_cpu</a></code></li>
<li><code><a title="ilpyt.agents.gail_agent.GAILAgent.to_gpu" href="#ilpyt.agents.gail_agent.GAILAgent.to_gpu">to_gpu</a></code></li>
<li><code><a title="ilpyt.agents.gail_agent.GAILAgent.update" href="#ilpyt.agents.gail_agent.GAILAgent.update">update</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>