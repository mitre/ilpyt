<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ilpyt API documentation</title>
<meta name="description" content="ilpyt â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>ilpyt</code></h1>
</header>
<section id="section-intro">
<h1 id="ilpyt">ilpyt</h1>
<p>The imitation learning toolbox (<code><a title="ilpyt" href="#ilpyt">ilpyt</a></code>) contains modular implementations of
common deep imitation learning algorithms in PyTorch, with unified
infrastructure supporting key imitation learning and reinforcement learning
algorithms. You can read more about <code><a title="ilpyt" href="#ilpyt">ilpyt</a></code> in our
<a href="https://github.com/mitre/ilpyt/docs/ilpyt_white_paper.pdf">white paper</a>.</p>
<p>Documentation is available <a href="https://mitre.github.io/ilpyt">here</a>.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#main-features">Main Features</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#basic-usage">Getting Started: Basic Usage</a></li>
<li><a href="#code-organization">Getting Started: Code Organization</a> </li>
<li><a href="#customization">Getting Started: Customization</a></li>
<li><a href="#supported-algorithms-and-environments">Supported Algorithms and Environments</a></li>
<li><a href="#benchmarks">Benchmarks</a></li>
<li><a href="#citation">Citation</a></li>
</ul>
<h2 id="main-features">Main Features</h2>
<ul>
<li>Implementation of baseline imitation learning algorithms: BC, DAgger, Apprenticeship learning, GCL, GAIL.</li>
<li>Implementation of baseline reinforcement learning algorithms, for comparison purposes: DQN, A2C, PPO.</li>
<li>Modular, extensible framework for training, evaluating, and testing imitation learning (and reinforcement learning) algorithms.</li>
<li>Simple algorithm API which exposes train and test methods, allowing for quick library setup and use (a basic usage of the library requires less than ten lines of code to have a fully functioning train and test pipeline).</li>
<li>A modular infrastructure for easy modification and reuse of existing components for novel algorithm implementations.</li>
<li>Parallel and serialization modes, allowing for faster, optimized operations or serial operations for debugging.</li>
<li>Compatibility with the OpenAI Gym environment interface for access to many existing benchmark learning environments, as well as the flexibility to create custom environments.</li>
</ul>
<h2 id="installation">Installation</h2>
<p>Note: <code><a title="ilpyt" href="#ilpyt">ilpyt</a></code> has only been tested on Ubuntu 20.04, and with Python 3.8.5. </p>
<p><strong>STEP 1</strong> </p>
<p>In order to install <code><a title="ilpyt" href="#ilpyt">ilpyt</a></code>, there are a few prerequisites required. The following commands will setup all the basics so you can run <code><a title="ilpyt" href="#ilpyt">ilpyt</a></code> with the OpenAI Gym environments:</p>
<pre><code># Install system-based packages
apt-get install cmake python3-pip python3-testresources freeglut3-dev xvfb

# Install Wheel
pip3 install --no-cache-dir --no-warn-script-location wheel
</code></pre>
<p><strong>STEP 2</strong> </p>
<p>Install <code><a title="ilpyt" href="#ilpyt">ilpyt</a></code> using pip:</p>
<pre><code>pip3 install ilpyt
</code></pre>
<p><strong>STEP 3</strong> (Optional) </p>
<p>Run the associated Python tests to confirm the package has
installed successfully: </p>
<pre><code>git clone https://github.com/mitre/ilpyt.git
cd ilpyt/

# To run all the tests
# If running headless, prepend the pytest command with `xvfb-run -a -s &quot;-screen 0 1400x900x24 +extension RANDR&quot; --`
pytest tests/

# Example: to run an individual test, like DQN
pytest tests/test_dqn.py 
</code></pre>
<h2 id="getting-started">Getting Started</h2>
<p>Various sample Python script(s) of how to run the toolbox can be found within
the <code>examples</code> directory. Documentation is available
<a href="https://mitre.github.io/ilpyt">here</a>.</p>
<h3 id="basic-usage">Basic Usage</h3>
<p>Various sample Python script(s) of how to run the toolbox can be found within
the <code>examples</code> directory. A minimal train and test snippet for an imitation
learning algorithm takes less than 10 lines of code in <code><a title="ilpyt" href="#ilpyt">ilpyt</a></code>. In this basic
example, we are training a behavioral cloning algorithm for 10,000 epochs before
testing the best policy for 100 episodes.</p>
<pre><code class="language-py">import ilpyt
from ilpyt.agents.imitation_agent import ImitationAgent
from ilpyt.algos.bc import BC

env = ilpyt.envs.build_env(env_id='LunarLander-v2',  num_env=16)
net = ilpyt.nets.choose_net(env)
agent = ImitationAgent(net=net, lr=0.0001)

algo = BC(agent=agent, env=env)
algo.train(num_epochs=10000, expert_demos='demos/LunarLander-v2/demos.pkl')
algo.test(num_episodes=100)
</code></pre>
<h3 id="code-organization">Code Organization</h3>
<p>The main components of the <code><a title="ilpyt" href="#ilpyt">ilpyt</a></code> library are the <code>algorithm</code>, <code>environment</code>,
<code>agent</code>, <code>net</code>, and <code>runner</code>. Please see our
<a href="https://github.com/mitre/ilpyt/docs/figures/conceptual_structure.png">conceptual structure diagram</a>
for more details.</p>
<p>At a high-level, the <code>algorithm</code> orchestrates the training and testing of our
<code>agent</code> in a particular <code>environment</code>. During these training or testing loops,
a <code>runner</code> will execute the <code>agent</code> and <code>environment</code> in a loop to collect
(<code>state</code>, <code>action</code>, <code>reward</code>, <code>next state</code>) transitions. The individual
components of a transition (e.g., <code>state</code> or <code>action</code>) are typically torch
Tensors. The <code>agent</code> can then use this batch of transitions to update its
<code>network</code> and move towards an optimal action policy.</p>
<h3 id="customization">Customization</h3>
<p>To implement a new algorithm, one simply has to extend the BaseAlgorithm and
BaseAgent abstract classes (for even further customization, one can even make
custom networks by extending the BaseNetwork interface). Each of these
components is modular (see <a href="#code-organization">code organization</a> for more
details), allowing components to be easily swapped out. (For example, the
agent.generator used in the GAIL algorithm can be easily swapped between
PPOAgent, DQNAgent, or A2Cagent. In a similar way, new algorithm implementations
can utilize existing implemented classes as building blocks, or extend the class
interfaces for more customization.)</p>
<p>Adding a custom environment is as simple as extending the OpenAI Gym Environment
interface and registering it within your local gym environment registry.</p>
<p>See <code>agents/base_agent.py</code>, <code>algos/base_algo.py</code>, <code>nets/base_net.py</code> for more
details.</p>
<h2 id="supported-algorithms-and-environments">Supported Algorithms and Environments</h2>
<p>The following imitation learning (IL) algorithms are supported:</p>
<ul>
<li><a href="https://papers.nips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf">Behavioral Cloning</a> (BC)</li>
<li><a href="https://arxiv.org/pdf/1011.0686.pdf">Dataset Aggregation</a> (DAgger)</li>
<li><a href="https://arxiv.org/abs/1606.03476">Generative Adversarial Imitation Learning</a> (GAIL)</li>
<li><a href="https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf">Apprenticeship Learning</a> (AppL)</li>
<li><a href="https://arxiv.org/abs/1603.00448">Guided Cost Learning</a> (GCL)</li>
</ul>
<p>The following reinforcement learning (RL) algorithms are supported:</p>
<ul>
<li><a href="https://arxiv.org/abs/1602.01783">Advantage Actor Critic</a> (A2C)</li>
<li><a href="https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning">Deep Q-Networks</a> (DQN)</li>
<li><a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization</a> (PPO)</li>
</ul>
<p>The following <a href="https://github.com/openai/gym/wiki/Table-of-environments">OpenAI Gym Environments</a> are supported. Environments with:</p>
<ul>
<li>Observation space: <code>Box(x,)</code> and <code>Box(x,y,z)</code></li>
<li>Action space: <code>Discrete(x)</code> and <code>Box(x,)</code></li>
</ul>
<p>NOTE: To create your own custom environment, just follow the OpenAI Gym
Environment interface. i.e., your environment must implement the following
methods (and inherit from the OpenAI Gym Class). More detailed instructions can
be found on the OpenAI GitHub repository page on creating
<a href="https://github.com/openai/gym/blob/master/docs/creating-environments.md">custom Gym environments</a>.</p>
<h2 id="benchmarks">Benchmarks</h2>
<p>Sample train and test results of the baseline algorithms on some environments:</p>
<table>
<thead>
<tr>
<th></th>
<th>CartPole-v0</th>
<th>MountainCar-v0</th>
<th>MountainCarContinuous-v0</th>
<th>LunarLander-v2</th>
<th>LunarLanderContinuous-v2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Threshold</td>
<td>200</td>
<td>-110</td>
<td>90</td>
<td>200</td>
<td>200</td>
</tr>
<tr>
<td>Expert (Mean/Std)</td>
<td>200.00 / 0.00</td>
<td>-98.71 / 7.83</td>
<td>93.36 / 0.05</td>
<td>268.09 / 21.18</td>
<td>283.83 / 17.70</td>
</tr>
<tr>
<td>BC (Mean/Std)</td>
<td>200.00 / 0.00</td>
<td>-100.800 / 13.797</td>
<td>93.353 / 0.113</td>
<td>244.295 / 97.765</td>
<td>285.895 / 14.584</td>
</tr>
<tr>
<td>DAgger (Mean/Std)</td>
<td>200.00 / 0.00</td>
<td>-102.36 / 15.38</td>
<td>93.20 / 0.17</td>
<td>230.15 / 122.604</td>
<td>285.85 / 14.61</td>
</tr>
<tr>
<td>GAIL (Mean/Std)</td>
<td>200.00 / 0.00</td>
<td>-104.31 / 17.21</td>
<td>79.78 / 6.23</td>
<td>201.88 / 93.82</td>
<td>282.00 / 31.73</td>
</tr>
<tr>
<td>GCL</td>
<td>200.00 / 0.00</td>
<td>-</td>
<td>-</td>
<td>212.321 / 119.933</td>
<td>255.414 / 76.917</td>
</tr>
<tr>
<td>AppL(Mean/Std)</td>
<td>200.00 / 0.00</td>
<td>-108.60 / 22.843</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>DQN (Mean/Std)</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>281.96 / 24.57</td>
<td>-</td>
</tr>
<tr>
<td>A2C (Mean/Std)</td>
<td>-</td>
<td></td>
<td>-</td>
<td>201.26 / 62.52</td>
<td>-</td>
</tr>
<tr>
<td>PPO (Mean/Std)</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>249.72 / 75.05</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>The pre-trained weights for these models can be found in our
<a href="https://github.com/mitre/ilpyt/model_zoo">Model Zoo</a>.</p>
<h2 id="citation">Citation</h2>
<p>If you use <code><a title="ilpyt" href="#ilpyt">ilpyt</a></code> for your work, please cite our
<a href="https://github.com/mitre/ilpyt/docs/ilpyt_white_paper.pdf">white paper</a>:</p>
<pre><code>@misc{ilpyt_2021,
  author = {Vu, Amanda and Tapley, Alex and Bissey, Brett},
  title = {ilpyt: Imitation Learning Research Code Base in PyTorch},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/mitre/ilpyt}},
}
</code></pre>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
# ilpyt

The imitation learning toolbox (`ilpyt`) contains modular implementations of 
common deep imitation learning algorithms in PyTorch, with unified 
infrastructure supporting key imitation learning and reinforcement learning 
algorithms. You can read more about `ilpyt` in our 
[white paper](https://github.com/mitre/ilpyt/docs/ilpyt_white_paper.pdf).

Documentation is available [here](https://mitre.github.io/ilpyt).

## Table of Contents
- [Main Features](#main-features)
- [Installation](#installation)
- [Getting Started](#getting-started)
- [Getting Started: Basic Usage](#basic-usage)
- [Getting Started: Code Organization](#code-organization) 
- [Getting Started: Customization](#customization)
- [Supported Algorithms and Environments](#supported-algorithms-and-environments)
- [Benchmarks](#benchmarks)
- [Citation](#citation)

## Main Features

- Implementation of baseline imitation learning algorithms: BC, DAgger, Apprenticeship learning, GCL, GAIL.
- Implementation of baseline reinforcement learning algorithms, for comparison purposes: DQN, A2C, PPO.
- Modular, extensible framework for training, evaluating, and testing imitation learning (and reinforcement learning) algorithms.
- Simple algorithm API which exposes train and test methods, allowing for quick library setup and use (a basic usage of the library requires less than ten lines of code to have a fully functioning train and test pipeline).
- A modular infrastructure for easy modification and reuse of existing components for novel algorithm implementations.
- Parallel and serialization modes, allowing for faster, optimized operations or serial operations for debugging.
- Compatibility with the OpenAI Gym environment interface for access to many existing benchmark learning environments, as well as the flexibility to create custom environments.

## Installation

Note: `ilpyt` has only been tested on Ubuntu 20.04, and with Python 3.8.5. 

**STEP 1** 

In order to install `ilpyt`, there are a few prerequisites required. The following commands will setup all the basics so you can run `ilpyt` with the OpenAI Gym environments:

```
# Install system-based packages
apt-get install cmake python3-pip python3-testresources freeglut3-dev xvfb

# Install Wheel
pip3 install --no-cache-dir --no-warn-script-location wheel
```

**STEP 2** 

Install `ilpyt` using pip:

```
pip3 install ilpyt
```
**STEP 3** (Optional) 

Run the associated Python tests to confirm the package has 
installed successfully: 

```
git clone https://github.com/mitre/ilpyt.git
cd ilpyt/

# To run all the tests
# If running headless, prepend the pytest command with `xvfb-run -a -s &#34;-screen 0 1400x900x24 +extension RANDR&#34; --`
pytest tests/

# Example: to run an individual test, like DQN
pytest tests/test_dqn.py 
```

## Getting Started

Various sample Python script(s) of how to run the toolbox can be found within 
the `examples` directory. Documentation is available 
[here](https://mitre.github.io/ilpyt).

### Basic Usage

Various sample Python script(s) of how to run the toolbox can be found within 
the `examples` directory. A minimal train and test snippet for an imitation 
learning algorithm takes less than 10 lines of code in `ilpyt`. In this basic 
example, we are training a behavioral cloning algorithm for 10,000 epochs before 
testing the best policy for 100 episodes.

```py
import ilpyt
from ilpyt.agents.imitation_agent import ImitationAgent
from ilpyt.algos.bc import BC

env = ilpyt.envs.build_env(env_id=&#39;LunarLander-v2&#39;,  num_env=16)
net = ilpyt.nets.choose_net(env)
agent = ImitationAgent(net=net, lr=0.0001)

algo = BC(agent=agent, env=env)
algo.train(num_epochs=10000, expert_demos=&#39;demos/LunarLander-v2/demos.pkl&#39;)
algo.test(num_episodes=100)
```

### Code Organization 

The main components of the `ilpyt` library are the `algorithm`, `environment`, 
`agent`, `net`, and `runner`. Please see our 
[conceptual structure diagram](https://github.com/mitre/ilpyt/docs/figures/conceptual_structure.png) 
for more details.

At a high-level, the `algorithm` orchestrates the training and testing of our 
`agent` in a particular `environment`. During these training or testing loops, 
a `runner` will execute the `agent` and `environment` in a loop to collect 
(`state`, `action`, `reward`, `next state`) transitions. The individual 
components of a transition (e.g., `state` or `action`) are typically torch 
Tensors. The `agent` can then use this batch of transitions to update its 
`network` and move towards an optimal action policy.

### Customization

To implement a new algorithm, one simply has to extend the BaseAlgorithm and 
BaseAgent abstract classes (for even further customization, one can even make 
custom networks by extending the BaseNetwork interface). Each of these 
components is modular (see [code organization](#code-organization) for more 
details), allowing components to be easily swapped out. (For example, the 
agent.generator used in the GAIL algorithm can be easily swapped between 
PPOAgent, DQNAgent, or A2Cagent. In a similar way, new algorithm implementations 
can utilize existing implemented classes as building blocks, or extend the class 
interfaces for more customization.)

Adding a custom environment is as simple as extending the OpenAI Gym Environment 
interface and registering it within your local gym environment registry.

See `agents/base_agent.py`, `algos/base_algo.py`, `nets/base_net.py` for more 
details.

## Supported Algorithms and Environments

The following imitation learning (IL) algorithms are supported:

* [Behavioral Cloning](https://papers.nips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf) (BC)
* [Dataset Aggregation](https://arxiv.org/pdf/1011.0686.pdf) (DAgger)
* [Generative Adversarial Imitation Learning](https://arxiv.org/abs/1606.03476) (GAIL)
* [Apprenticeship Learning](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf) (AppL)
* [Guided Cost Learning](https://arxiv.org/abs/1603.00448) (GCL)

The following reinforcement learning (RL) algorithms are supported:

* [Advantage Actor Critic](https://arxiv.org/abs/1602.01783) (A2C)
* [Deep Q-Networks](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning) (DQN)
* [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) (PPO)

The following [OpenAI Gym Environments](https://github.com/openai/gym/wiki/Table-of-environments) are supported. Environments with:

* Observation space: `Box(x,)` and `Box(x,y,z)`
* Action space: `Discrete(x)` and `Box(x,)`

NOTE: To create your own custom environment, just follow the OpenAI Gym 
Environment interface. i.e., your environment must implement the following 
methods (and inherit from the OpenAI Gym Class). More detailed instructions can 
be found on the OpenAI GitHub repository page on creating 
[custom Gym environments](https://github.com/openai/gym/blob/master/docs/creating-environments.md).

## Benchmarks

Sample train and test results of the baseline algorithms on some environments:

| |CartPole-v0 | MountainCar-v0 | MountainCarContinuous-v0 | LunarLander-v2 | LunarLanderContinuous-v2 |
| -- | -- | -- | -- | -- | -- | 
| Threshold | 200 | -110  |  90 | 200 |  200 | 
Expert (Mean/Std) |  200.00 / 0.00 | -98.71 / 7.83 |  93.36 / 0.05 | 268.09 / 21.18 |  283.83 / 17.70 | 
BC (Mean/Std) |  200.00 / 0.00 | -100.800 / 13.797 | 93.353 / 0.113 | 244.295 / 97.765 |  285.895 / 14.584 |
DAgger (Mean/Std) | 200.00 / 0.00 | -102.36 / 15.38 |  93.20 / 0.17 |  230.15 / 122.604 | 285.85 / 14.61 | 
GAIL (Mean/Std) | 200.00 / 0.00 | -104.31 / 17.21 |  79.78 / 6.23 |  201.88 / 93.82 | 282.00 / 31.73 |
GCL | 200.00 / 0.00 | - | - | 212.321 / 119.933 | 255.414 / 76.917 |
AppL(Mean/Std) |   200.00 / 0.00 | -108.60 / 22.843 | -  | -  | -  |
DQN (Mean/Std) |  - | - |  - |  281.96 / 24.57 | - |
A2C (Mean/Std) |  - |  | - | 201.26 / 62.52 |   - |
PPO (Mean/Std) |  - | - | -  | 249.72 / 75.05 | -  |

The pre-trained weights for these models can be found in our 
[Model Zoo](https://github.com/mitre/ilpyt/model_zoo).

## Citation

If you use `ilpyt` for your work, please cite our 
[white paper](https://github.com/mitre/ilpyt/docs/ilpyt_white_paper.pdf):
```
@misc{ilpyt_2021,
  author = {Vu, Amanda and Tapley, Alex and Bissey, Brett},
  title = {ilpyt: Imitation Learning Research Code Base in PyTorch},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\\url{https://github.com/mitre/ilpyt}},
}
```
&#34;&#34;&#34;</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="ilpyt.agents" href="agents/index.html">ilpyt.agents</a></code></dt>
<dd>
<div class="desc"><p>An agent's role during learning is to coordinate the policy learning and
execution. Here, the policy refers to a function (in this case, a deep â€¦</p></div>
</dd>
<dt><code class="name"><a title="ilpyt.algos" href="algos/index.html">ilpyt.algos</a></code></dt>
<dd>
<div class="desc"><p>Algorithms are the main entrypoint for the ilpyt library. An algorithm's role
during learning is to perform the high-level coordinatination between â€¦</p></div>
</dd>
<dt><code class="name"><a title="ilpyt.envs" href="envs/index.html">ilpyt.envs</a></code></dt>
<dd>
<div class="desc"><p>Wrappers for the OpenAI Gym environments:<code>SubProcVecEnv</code> and <code>DummyVecEnv</code>. They produce parallelized and serializedvectorized Gym environments for â€¦</p></div>
</dd>
<dt><code class="name"><a title="ilpyt.nets" href="nets/index.html">ilpyt.nets</a></code></dt>
<dd>
<div class="desc"><p>Wrappers for the <code>torch.nn.Module</code> class, and can be used to parameterize policy
functions, value functions, cost functions, etc. for use in â€¦</p></div>
</dd>
<dt><code class="name"><a title="ilpyt.runners" href="runners/index.html">ilpyt.runners</a></code></dt>
<dd>
<div class="desc"><p>The runner coordinates the interaction between the agent and the environment.
It collects transitions (state, action, reward, next state) over â€¦</p></div>
</dd>
<dt><code class="name"><a title="ilpyt.utils" href="utils/index.html">ilpyt.utils</a></code></dt>
<dd>
<div class="desc"><p>Utility functions for the library.</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#ilpyt">ilpyt</a><ul>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#main-features">Main Features</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#getting-started">Getting Started</a><ul>
<li><a href="#basic-usage">Basic Usage</a></li>
<li><a href="#code-organization">Code Organization</a></li>
<li><a href="#customization">Customization</a></li>
</ul>
</li>
<li><a href="#supported-algorithms-and-environments">Supported Algorithms and Environments</a></li>
<li><a href="#benchmarks">Benchmarks</a></li>
<li><a href="#citation">Citation</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="ilpyt.agents" href="agents/index.html">ilpyt.agents</a></code></li>
<li><code><a title="ilpyt.algos" href="algos/index.html">ilpyt.algos</a></code></li>
<li><code><a title="ilpyt.envs" href="envs/index.html">ilpyt.envs</a></code></li>
<li><code><a title="ilpyt.nets" href="nets/index.html">ilpyt.nets</a></code></li>
<li><code><a title="ilpyt.runners" href="runners/index.html">ilpyt.runners</a></code></li>
<li><code><a title="ilpyt.utils" href="utils/index.html">ilpyt.utils</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>