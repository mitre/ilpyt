<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ilpyt.agents.dqn_agent API documentation</title>
<meta name="description" content="An implementation of the agent from the Deep Q-Networks (DQN) algorithm. This
algorithm was described in the paper &#34;Human Level Control Through Deep
…" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ilpyt.agents.dqn_agent</code></h1>
</header>
<section id="section-intro">
<p>An implementation of the agent from the Deep Q-Networks (DQN) algorithm. This
algorithm was described in the paper "Human Level Control Through Deep
Reinforcement Learning" by Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller,
Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie,
Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis, and published in Nature in February 2015. </p>
<p>For more details, please refer to the paper: <a href="https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning">https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
An implementation of the agent from the Deep Q-Networks (DQN) algorithm. This 
algorithm was described in the paper &#34;Human Level Control Through Deep 
Reinforcement Learning&#34; by Volodymyr Mnih, Koray Kavukcuoglu, David Silver, 
Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, 
Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, 
Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, 
Shane Legg, and Demis Hassabis, and published in Nature in February 2015. 

For more details, please refer to the paper: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning
&#34;&#34;&#34;

import random
from typing import Dict, Union

import numpy as np
import torch
import torch.nn.functional as F
from torch.optim import Adam

from ilpyt.agents.base_agent import BaseAgent
from ilpyt.nets.base_net import BaseNetwork
from ilpyt.utils.agent_utils import flatten_batch, hard_update, soft_update
from ilpyt.utils.replay_memory import ReplayMemory


class DQNAgent(BaseAgent):
    def initialize(
        self,
        net: Union[BaseNetwork, None] = None,
        target_net: Union[BaseNetwork, None] = None,
        num_actions: int = -1,
        lr: float = 5e-5,
        replay_memory_size: int = int(1e4),
        epsilon_start: float = 0.95,
        epsilon_end: float = 0.01,
        epsilon_steps: int = int(1e5),
        tau: float = 0.01,
        gamma: float = 0.99,
        batch_size: int = 64,
        num_envs: int = 16,
    ) -&gt; None:
        &#34;&#34;&#34;
        Initialization function for the DQN Agent.

        Parameters
        ----------
        net: BaseNetwork, default=None
            deep q-network
        target_net: BaseNetwork, default=None
            target deep q-network
        replay_memory_size: int, default=1e4
            number of samples to store in replay memory
        num_actions: int, default=-1
            number of possible actions
        lr: float, default=5e-5
            learning rate
        epsilon_start: float, default=0.95
            probability between [0,1] of when to choose a random action
        epsilon_end: float, default=0.01
            probability between [0,1] of when to choose a random action
        epsilon_steps: int, default=1e5
            umber of steps to decrease from epsilon_start to epsilon_end
        tau: float, default=0.01
            soft update for target network [0, 1]
        gamma: float, default=0.99
            discount factor for estimating returns[0, 1]
        batch_size: int, default=64
            number of samples to take from replay_memory for a network update

        Raises
        ------
        ValueError:
            If `net` or `target_net` are not specified.
            If `num_actions` is not specified.
        &#34;&#34;&#34;
        if num_actions == -1:
            raise ValueError(
                &#39;Please provide valid input value for num_actions (positive integer). Currently set to -1.&#39;
            )
        self.gamma = gamma
        self.tau = tau
        self.batch_size = batch_size
        self.num_actions = num_actions
        self.num_envs = num_envs

        if net is None or target_net is None:
            raise ValueError(
                &#39;Please provide input value for net and target_net. Currently set to None.&#39;
            )
        self.net = net
        self.target_net = target_net
        hard_update(self.net, self.target_net)
        self.nets = {&#39;net&#39;: self.net, &#39;target&#39;: self.target_net}

        self.lr = lr
        # Optimizer
        self.opt = Adam(self.net.parameters(), lr=self.lr)

        self.replay_memory_size = replay_memory_size
        # Replay memory
        self.memory = ReplayMemory(self.replay_memory_size)

        # Epsilon used for selecting actions
        self.epsilon_start = epsilon_start
        self.epsilon = epsilon_start
        self.epsilon_step = (epsilon_start - epsilon_end) / epsilon_steps
        self.epsilon_end = epsilon_end

    @torch.no_grad()
    def reset(self) -&gt; None:
        &#34;&#34;&#34;
        Reset the DQN agent network weights, replay memory, optimizers, and 
        epsilon.
        &#34;&#34;&#34;
        # Reset net weights
        for layers in self.net.children():
            for layer in layers:
                if hasattr(layer, &#39;reset_parameters&#39;):
                    layer.reset_parameters()

        hard_update(self.net, self.target_net)
        self.memory = ReplayMemory(self.replay_memory_size)

        # Reset optimizers
        self.opt = Adam(self.net.parameters(), self.lr)

        # Reset epsilon
        self.epsilon = self.epsilon_start

    @torch.no_grad()
    def step(self, state: torch.Tensor) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Find best action for the given state.

        Perform a random action with probability `self.epsilon`. Otherwise, 
        select the action which yields the maximum reward according to the
        current policy.

        Parameters
        ----------
        state: torch.Tensor
            state tensor, of size (batch_size, state_shape)

        Returns
        -------
        np.ndarray:
            selected actions, of size (batch_size, action_shape)
        &#34;&#34;&#34;
        # Select epsilon
        self.epsilon = max(self.epsilon - self.epsilon_step, self.epsilon_end)

        # Perform random action with probability self.epsilon. Otherwise, select
        # the action which yields the maximum reward.
        if random.random() &lt;= self.epsilon and self.mode == &#39;train&#39;:
            batch_size = state.shape[0]
            actions = np.random.choice(self.num_actions, batch_size)
        else:
            action_logits = self.net(state)
            actions = torch.argmax(action_logits, dim=-1)
            if self.device == &#39;gpu&#39;:
                actions = actions.cpu().numpy()
            else:
                actions = actions.numpy()
        return actions

    def update(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, float]:
        &#34;&#34;&#34;
        Update agent policy based on batch of experiences.

        Parameters
        ----------
        batch: Dict[str, torch.Tensor]
            batch of transitions, with keys `states`, `actions`, `rewards`, 
            `dones`, and `next_states`. Values should be of size (num_steps, 
            num_env, item_shape)

        Returns
        -------
        Dict[str, float]:
            losses for the update step, key strings and loss values can be
            automatically recorded to TensorBoard
        &#34;&#34;&#34;
        # Add to replay memory
        chunk_states = torch.chunk(batch[&#39;states&#39;], self.num_envs, dim=1)
        chunk_next_states = torch.chunk(
            batch[&#39;next_states&#39;], self.num_envs, dim=1
        )
        chunk_actions = torch.chunk(batch[&#39;actions&#39;], self.num_envs, dim=1)
        chunk_rewards = torch.chunk(batch[&#39;rewards&#39;], self.num_envs, dim=1)
        chunk_dones = torch.chunk(batch[&#39;dones&#39;], self.num_envs, dim=1)

        for i in range(self.num_envs):
            rollout = {
                &#39;states&#39;: chunk_states[i],
                &#39;next_states&#39;: chunk_next_states[1],
                &#39;actions&#39;: chunk_actions[i],
                &#39;rewards&#39;: chunk_rewards[i],
                &#39;dones&#39;: chunk_dones[i],
                &#39;infos&#39;: [],
            }

            for ep_count, info_dict in batch[&#39;infos&#39;]:
                for (k, _) in info_dict.items():
                    if &#39;reward&#39; in k and int(k.split(&#39;/&#39;)[1]) == i:
                        rollout[&#39;infos&#39;].append([ep_count, info_dict])
            flattened = flatten_batch(rollout)
            self.memory.add(flattened)

        # Sample a batch
        batch = self.memory.sample(self.batch_size)
        if batch is None:
            return {}
        # Compute Q-values
        actions = batch[&#39;actions&#39;].to(torch.int64).unsqueeze(1)
        action_logits = self.net(batch[&#39;states&#39;])
        qs = action_logits.gather(-1, actions).squeeze()

        # Compute targets
        masks = 1 - batch[&#39;dones&#39;]
        target_action_logits = self.target_net(batch[&#39;next_states&#39;]).detach()
        target_max_action_logits = torch.max(
            target_action_logits, dim=-1
        ).values.detach()
        q_targets = (
            batch[&#39;rewards&#39;] + self.gamma * masks * target_max_action_logits
        )

        # Compute loss
        loss = F.mse_loss(qs, q_targets)

        # Optimize model
        self.opt.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.5)
        self.opt.step()

        # Update target
        soft_update(self.net, self.target_net, self.tau)

        # Return loss dictionary
        loss_dict = {&#39;loss/total&#39;: loss.item()}
        return loss_dict</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ilpyt.agents.dqn_agent.DQNAgent"><code class="flex name class">
<span>class <span class="ident">DQNAgent</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>By default, the agent will be in <code>train</code> mode and be configured to use
the <code>cpu</code> for <code>step</code> and <code>update</code> calls.</p>
<h2 id="parameters">Parameters</h2>
<p>**kwargs:
arbitrary keyword arguments that will be passed to the <code>initialize</code> function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DQNAgent(BaseAgent):
    def initialize(
        self,
        net: Union[BaseNetwork, None] = None,
        target_net: Union[BaseNetwork, None] = None,
        num_actions: int = -1,
        lr: float = 5e-5,
        replay_memory_size: int = int(1e4),
        epsilon_start: float = 0.95,
        epsilon_end: float = 0.01,
        epsilon_steps: int = int(1e5),
        tau: float = 0.01,
        gamma: float = 0.99,
        batch_size: int = 64,
        num_envs: int = 16,
    ) -&gt; None:
        &#34;&#34;&#34;
        Initialization function for the DQN Agent.

        Parameters
        ----------
        net: BaseNetwork, default=None
            deep q-network
        target_net: BaseNetwork, default=None
            target deep q-network
        replay_memory_size: int, default=1e4
            number of samples to store in replay memory
        num_actions: int, default=-1
            number of possible actions
        lr: float, default=5e-5
            learning rate
        epsilon_start: float, default=0.95
            probability between [0,1] of when to choose a random action
        epsilon_end: float, default=0.01
            probability between [0,1] of when to choose a random action
        epsilon_steps: int, default=1e5
            umber of steps to decrease from epsilon_start to epsilon_end
        tau: float, default=0.01
            soft update for target network [0, 1]
        gamma: float, default=0.99
            discount factor for estimating returns[0, 1]
        batch_size: int, default=64
            number of samples to take from replay_memory for a network update

        Raises
        ------
        ValueError:
            If `net` or `target_net` are not specified.
            If `num_actions` is not specified.
        &#34;&#34;&#34;
        if num_actions == -1:
            raise ValueError(
                &#39;Please provide valid input value for num_actions (positive integer). Currently set to -1.&#39;
            )
        self.gamma = gamma
        self.tau = tau
        self.batch_size = batch_size
        self.num_actions = num_actions
        self.num_envs = num_envs

        if net is None or target_net is None:
            raise ValueError(
                &#39;Please provide input value for net and target_net. Currently set to None.&#39;
            )
        self.net = net
        self.target_net = target_net
        hard_update(self.net, self.target_net)
        self.nets = {&#39;net&#39;: self.net, &#39;target&#39;: self.target_net}

        self.lr = lr
        # Optimizer
        self.opt = Adam(self.net.parameters(), lr=self.lr)

        self.replay_memory_size = replay_memory_size
        # Replay memory
        self.memory = ReplayMemory(self.replay_memory_size)

        # Epsilon used for selecting actions
        self.epsilon_start = epsilon_start
        self.epsilon = epsilon_start
        self.epsilon_step = (epsilon_start - epsilon_end) / epsilon_steps
        self.epsilon_end = epsilon_end

    @torch.no_grad()
    def reset(self) -&gt; None:
        &#34;&#34;&#34;
        Reset the DQN agent network weights, replay memory, optimizers, and 
        epsilon.
        &#34;&#34;&#34;
        # Reset net weights
        for layers in self.net.children():
            for layer in layers:
                if hasattr(layer, &#39;reset_parameters&#39;):
                    layer.reset_parameters()

        hard_update(self.net, self.target_net)
        self.memory = ReplayMemory(self.replay_memory_size)

        # Reset optimizers
        self.opt = Adam(self.net.parameters(), self.lr)

        # Reset epsilon
        self.epsilon = self.epsilon_start

    @torch.no_grad()
    def step(self, state: torch.Tensor) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Find best action for the given state.

        Perform a random action with probability `self.epsilon`. Otherwise, 
        select the action which yields the maximum reward according to the
        current policy.

        Parameters
        ----------
        state: torch.Tensor
            state tensor, of size (batch_size, state_shape)

        Returns
        -------
        np.ndarray:
            selected actions, of size (batch_size, action_shape)
        &#34;&#34;&#34;
        # Select epsilon
        self.epsilon = max(self.epsilon - self.epsilon_step, self.epsilon_end)

        # Perform random action with probability self.epsilon. Otherwise, select
        # the action which yields the maximum reward.
        if random.random() &lt;= self.epsilon and self.mode == &#39;train&#39;:
            batch_size = state.shape[0]
            actions = np.random.choice(self.num_actions, batch_size)
        else:
            action_logits = self.net(state)
            actions = torch.argmax(action_logits, dim=-1)
            if self.device == &#39;gpu&#39;:
                actions = actions.cpu().numpy()
            else:
                actions = actions.numpy()
        return actions

    def update(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, float]:
        &#34;&#34;&#34;
        Update agent policy based on batch of experiences.

        Parameters
        ----------
        batch: Dict[str, torch.Tensor]
            batch of transitions, with keys `states`, `actions`, `rewards`, 
            `dones`, and `next_states`. Values should be of size (num_steps, 
            num_env, item_shape)

        Returns
        -------
        Dict[str, float]:
            losses for the update step, key strings and loss values can be
            automatically recorded to TensorBoard
        &#34;&#34;&#34;
        # Add to replay memory
        chunk_states = torch.chunk(batch[&#39;states&#39;], self.num_envs, dim=1)
        chunk_next_states = torch.chunk(
            batch[&#39;next_states&#39;], self.num_envs, dim=1
        )
        chunk_actions = torch.chunk(batch[&#39;actions&#39;], self.num_envs, dim=1)
        chunk_rewards = torch.chunk(batch[&#39;rewards&#39;], self.num_envs, dim=1)
        chunk_dones = torch.chunk(batch[&#39;dones&#39;], self.num_envs, dim=1)

        for i in range(self.num_envs):
            rollout = {
                &#39;states&#39;: chunk_states[i],
                &#39;next_states&#39;: chunk_next_states[1],
                &#39;actions&#39;: chunk_actions[i],
                &#39;rewards&#39;: chunk_rewards[i],
                &#39;dones&#39;: chunk_dones[i],
                &#39;infos&#39;: [],
            }

            for ep_count, info_dict in batch[&#39;infos&#39;]:
                for (k, _) in info_dict.items():
                    if &#39;reward&#39; in k and int(k.split(&#39;/&#39;)[1]) == i:
                        rollout[&#39;infos&#39;].append([ep_count, info_dict])
            flattened = flatten_batch(rollout)
            self.memory.add(flattened)

        # Sample a batch
        batch = self.memory.sample(self.batch_size)
        if batch is None:
            return {}
        # Compute Q-values
        actions = batch[&#39;actions&#39;].to(torch.int64).unsqueeze(1)
        action_logits = self.net(batch[&#39;states&#39;])
        qs = action_logits.gather(-1, actions).squeeze()

        # Compute targets
        masks = 1 - batch[&#39;dones&#39;]
        target_action_logits = self.target_net(batch[&#39;next_states&#39;]).detach()
        target_max_action_logits = torch.max(
            target_action_logits, dim=-1
        ).values.detach()
        q_targets = (
            batch[&#39;rewards&#39;] + self.gamma * masks * target_max_action_logits
        )

        # Compute loss
        loss = F.mse_loss(qs, q_targets)

        # Optimize model
        self.opt.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.5)
        self.opt.step()

        # Update target
        soft_update(self.net, self.target_net, self.tau)

        # Return loss dictionary
        loss_dict = {&#39;loss/total&#39;: loss.item()}
        return loss_dict</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ilpyt.agents.base_agent.BaseAgent" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent">BaseAgent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ilpyt.agents.dqn_agent.DQNAgent.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, net: Union[<a title="ilpyt.nets.base_net.BaseNetwork" href="../nets/base_net.html#ilpyt.nets.base_net.BaseNetwork">BaseNetwork</a>, NoneType] = None, target_net: Union[<a title="ilpyt.nets.base_net.BaseNetwork" href="../nets/base_net.html#ilpyt.nets.base_net.BaseNetwork">BaseNetwork</a>, NoneType] = None, num_actions: int = -1, lr: float = 5e-05, replay_memory_size: int = 10000, epsilon_start: float = 0.95, epsilon_end: float = 0.01, epsilon_steps: int = 100000, tau: float = 0.01, gamma: float = 0.99, batch_size: int = 64, num_envs: int = 16) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Initialization function for the DQN Agent.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>net</code></strong> :&ensp;<code>BaseNetwork</code>, default=<code>None</code></dt>
<dd>deep q-network</dd>
<dt><strong><code>target_net</code></strong> :&ensp;<code>BaseNetwork</code>, default=<code>None</code></dt>
<dd>target deep q-network</dd>
<dt><strong><code>replay_memory_size</code></strong> :&ensp;<code>int</code>, default=<code>1e4</code></dt>
<dd>number of samples to store in replay memory</dd>
<dt><strong><code>num_actions</code></strong> :&ensp;<code>int</code>, default=<code>-1</code></dt>
<dd>number of possible actions</dd>
<dt><strong><code>lr</code></strong> :&ensp;<code>float</code>, default=<code>5e-5</code></dt>
<dd>learning rate</dd>
<dt><strong><code>epsilon_start</code></strong> :&ensp;<code>float</code>, default=<code>0.95</code></dt>
<dd>probability between [0,1] of when to choose a random action</dd>
<dt><strong><code>epsilon_end</code></strong> :&ensp;<code>float</code>, default=<code>0.01</code></dt>
<dd>probability between [0,1] of when to choose a random action</dd>
<dt><strong><code>epsilon_steps</code></strong> :&ensp;<code>int</code>, default=<code>1e5</code></dt>
<dd>umber of steps to decrease from epsilon_start to epsilon_end</dd>
<dt><strong><code>tau</code></strong> :&ensp;<code>float</code>, default=<code>0.01</code></dt>
<dd>soft update for target network [0, 1]</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code>, default=<code>0.99</code></dt>
<dd>discount factor for estimating returns[0, 1]</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, default=<code>64</code></dt>
<dd>number of samples to take from replay_memory for a network update</dd>
</dl>
<h2 id="raises">Raises</h2>
<h2 id="valueerror">Valueerror</h2>
<p>If <code>net</code> or <code>target_net</code> are not specified.
If <code>num_actions</code> is not specified.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(
    self,
    net: Union[BaseNetwork, None] = None,
    target_net: Union[BaseNetwork, None] = None,
    num_actions: int = -1,
    lr: float = 5e-5,
    replay_memory_size: int = int(1e4),
    epsilon_start: float = 0.95,
    epsilon_end: float = 0.01,
    epsilon_steps: int = int(1e5),
    tau: float = 0.01,
    gamma: float = 0.99,
    batch_size: int = 64,
    num_envs: int = 16,
) -&gt; None:
    &#34;&#34;&#34;
    Initialization function for the DQN Agent.

    Parameters
    ----------
    net: BaseNetwork, default=None
        deep q-network
    target_net: BaseNetwork, default=None
        target deep q-network
    replay_memory_size: int, default=1e4
        number of samples to store in replay memory
    num_actions: int, default=-1
        number of possible actions
    lr: float, default=5e-5
        learning rate
    epsilon_start: float, default=0.95
        probability between [0,1] of when to choose a random action
    epsilon_end: float, default=0.01
        probability between [0,1] of when to choose a random action
    epsilon_steps: int, default=1e5
        umber of steps to decrease from epsilon_start to epsilon_end
    tau: float, default=0.01
        soft update for target network [0, 1]
    gamma: float, default=0.99
        discount factor for estimating returns[0, 1]
    batch_size: int, default=64
        number of samples to take from replay_memory for a network update

    Raises
    ------
    ValueError:
        If `net` or `target_net` are not specified.
        If `num_actions` is not specified.
    &#34;&#34;&#34;
    if num_actions == -1:
        raise ValueError(
            &#39;Please provide valid input value for num_actions (positive integer). Currently set to -1.&#39;
        )
    self.gamma = gamma
    self.tau = tau
    self.batch_size = batch_size
    self.num_actions = num_actions
    self.num_envs = num_envs

    if net is None or target_net is None:
        raise ValueError(
            &#39;Please provide input value for net and target_net. Currently set to None.&#39;
        )
    self.net = net
    self.target_net = target_net
    hard_update(self.net, self.target_net)
    self.nets = {&#39;net&#39;: self.net, &#39;target&#39;: self.target_net}

    self.lr = lr
    # Optimizer
    self.opt = Adam(self.net.parameters(), lr=self.lr)

    self.replay_memory_size = replay_memory_size
    # Replay memory
    self.memory = ReplayMemory(self.replay_memory_size)

    # Epsilon used for selecting actions
    self.epsilon_start = epsilon_start
    self.epsilon = epsilon_start
    self.epsilon_step = (epsilon_start - epsilon_end) / epsilon_steps
    self.epsilon_end = epsilon_end</code></pre>
</details>
</dd>
<dt id="ilpyt.agents.dqn_agent.DQNAgent.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Reset the DQN agent network weights, replay memory, optimizers, and
epsilon.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def reset(self) -&gt; None:
    &#34;&#34;&#34;
    Reset the DQN agent network weights, replay memory, optimizers, and 
    epsilon.
    &#34;&#34;&#34;
    # Reset net weights
    for layers in self.net.children():
        for layer in layers:
            if hasattr(layer, &#39;reset_parameters&#39;):
                layer.reset_parameters()

    hard_update(self.net, self.target_net)
    self.memory = ReplayMemory(self.replay_memory_size)

    # Reset optimizers
    self.opt = Adam(self.net.parameters(), self.lr)

    # Reset epsilon
    self.epsilon = self.epsilon_start</code></pre>
</details>
</dd>
<dt id="ilpyt.agents.dqn_agent.DQNAgent.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, state: torch.Tensor) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Find best action for the given state.</p>
<p>Perform a random action with probability <code>self.epsilon</code>. Otherwise,
select the action which yields the maximum reward according to the
current policy.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>state tensor, of size (batch_size, state_shape)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray:</code></dt>
<dd>selected actions, of size (batch_size, action_shape)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def step(self, state: torch.Tensor) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Find best action for the given state.

    Perform a random action with probability `self.epsilon`. Otherwise, 
    select the action which yields the maximum reward according to the
    current policy.

    Parameters
    ----------
    state: torch.Tensor
        state tensor, of size (batch_size, state_shape)

    Returns
    -------
    np.ndarray:
        selected actions, of size (batch_size, action_shape)
    &#34;&#34;&#34;
    # Select epsilon
    self.epsilon = max(self.epsilon - self.epsilon_step, self.epsilon_end)

    # Perform random action with probability self.epsilon. Otherwise, select
    # the action which yields the maximum reward.
    if random.random() &lt;= self.epsilon and self.mode == &#39;train&#39;:
        batch_size = state.shape[0]
        actions = np.random.choice(self.num_actions, batch_size)
    else:
        action_logits = self.net(state)
        actions = torch.argmax(action_logits, dim=-1)
        if self.device == &#39;gpu&#39;:
            actions = actions.cpu().numpy()
        else:
            actions = actions.numpy()
    return actions</code></pre>
</details>
</dd>
<dt id="ilpyt.agents.dqn_agent.DQNAgent.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, batch: Dict[str, torch.Tensor]) ‑> Dict[str, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Update agent policy based on batch of experiences.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>batch</code></strong> :&ensp;<code>Dict[str, torch.Tensor]</code></dt>
<dd>batch of transitions, with keys <code>states</code>, <code>actions</code>, <code>rewards</code>,
<code>dones</code>, and <code>next_states</code>. Values should be of size (num_steps,
num_env, item_shape)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, float]:</code></dt>
<dd>losses for the update step, key strings and loss values can be
automatically recorded to TensorBoard</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, float]:
    &#34;&#34;&#34;
    Update agent policy based on batch of experiences.

    Parameters
    ----------
    batch: Dict[str, torch.Tensor]
        batch of transitions, with keys `states`, `actions`, `rewards`, 
        `dones`, and `next_states`. Values should be of size (num_steps, 
        num_env, item_shape)

    Returns
    -------
    Dict[str, float]:
        losses for the update step, key strings and loss values can be
        automatically recorded to TensorBoard
    &#34;&#34;&#34;
    # Add to replay memory
    chunk_states = torch.chunk(batch[&#39;states&#39;], self.num_envs, dim=1)
    chunk_next_states = torch.chunk(
        batch[&#39;next_states&#39;], self.num_envs, dim=1
    )
    chunk_actions = torch.chunk(batch[&#39;actions&#39;], self.num_envs, dim=1)
    chunk_rewards = torch.chunk(batch[&#39;rewards&#39;], self.num_envs, dim=1)
    chunk_dones = torch.chunk(batch[&#39;dones&#39;], self.num_envs, dim=1)

    for i in range(self.num_envs):
        rollout = {
            &#39;states&#39;: chunk_states[i],
            &#39;next_states&#39;: chunk_next_states[1],
            &#39;actions&#39;: chunk_actions[i],
            &#39;rewards&#39;: chunk_rewards[i],
            &#39;dones&#39;: chunk_dones[i],
            &#39;infos&#39;: [],
        }

        for ep_count, info_dict in batch[&#39;infos&#39;]:
            for (k, _) in info_dict.items():
                if &#39;reward&#39; in k and int(k.split(&#39;/&#39;)[1]) == i:
                    rollout[&#39;infos&#39;].append([ep_count, info_dict])
        flattened = flatten_batch(rollout)
        self.memory.add(flattened)

    # Sample a batch
    batch = self.memory.sample(self.batch_size)
    if batch is None:
        return {}
    # Compute Q-values
    actions = batch[&#39;actions&#39;].to(torch.int64).unsqueeze(1)
    action_logits = self.net(batch[&#39;states&#39;])
    qs = action_logits.gather(-1, actions).squeeze()

    # Compute targets
    masks = 1 - batch[&#39;dones&#39;]
    target_action_logits = self.target_net(batch[&#39;next_states&#39;]).detach()
    target_max_action_logits = torch.max(
        target_action_logits, dim=-1
    ).values.detach()
    q_targets = (
        batch[&#39;rewards&#39;] + self.gamma * masks * target_max_action_logits
    )

    # Compute loss
    loss = F.mse_loss(qs, q_targets)

    # Optimize model
    self.opt.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.5)
    self.opt.step()

    # Update target
    soft_update(self.net, self.target_net, self.tau)

    # Return loss dictionary
    loss_dict = {&#39;loss/total&#39;: loss.item()}
    return loss_dict</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ilpyt.agents.base_agent.BaseAgent" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent">BaseAgent</a></b></code>:
<ul class="hlist">
<li><code><a title="ilpyt.agents.base_agent.BaseAgent.load" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent.load">load</a></code></li>
<li><code><a title="ilpyt.agents.base_agent.BaseAgent.save" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent.save">save</a></code></li>
<li><code><a title="ilpyt.agents.base_agent.BaseAgent.set_test" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent.set_test">set_test</a></code></li>
<li><code><a title="ilpyt.agents.base_agent.BaseAgent.set_train" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent.set_train">set_train</a></code></li>
<li><code><a title="ilpyt.agents.base_agent.BaseAgent.to_cpu" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent.to_cpu">to_cpu</a></code></li>
<li><code><a title="ilpyt.agents.base_agent.BaseAgent.to_gpu" href="base_agent.html#ilpyt.agents.base_agent.BaseAgent.to_gpu">to_gpu</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ilpyt.agents" href="index.html">ilpyt.agents</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ilpyt.agents.dqn_agent.DQNAgent" href="#ilpyt.agents.dqn_agent.DQNAgent">DQNAgent</a></code></h4>
<ul class="">
<li><code><a title="ilpyt.agents.dqn_agent.DQNAgent.initialize" href="#ilpyt.agents.dqn_agent.DQNAgent.initialize">initialize</a></code></li>
<li><code><a title="ilpyt.agents.dqn_agent.DQNAgent.reset" href="#ilpyt.agents.dqn_agent.DQNAgent.reset">reset</a></code></li>
<li><code><a title="ilpyt.agents.dqn_agent.DQNAgent.step" href="#ilpyt.agents.dqn_agent.DQNAgent.step">step</a></code></li>
<li><code><a title="ilpyt.agents.dqn_agent.DQNAgent.update" href="#ilpyt.agents.dqn_agent.DQNAgent.update">update</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>